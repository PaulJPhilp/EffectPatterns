# Module 3: Building Data Pipelines

Welcome to Module 3. In the real world, not all applications are simple request-response servers. Many need to process large files, handle paginated APIs, or perform complex, multi-step data transformations. This is the world of data pipelines.

This module will teach you how to use `Effect.Stream` to solve these challenges. You will learn how to create data streams from various sources, transform the data safely and concurrently, and run the pipeline to its destination, all while ensuring resource safety and resilience.

---

### Learning Path

Follow these patterns in order to build a comprehensive understanding of data pipelines, from source to sink.

#### Part 1: Creating a Pipeline (Sources)

1.  #### [Create a Stream from a List](./patterns/stream-from-iterable)

    **Goal**: Create a data pipeline from a simple in-memory array or list.
    This is the "Hello, World!" of streams, perfect for understanding the basics and for use in testing.

2.  #### [Process a Large File with Constant Memory](./patterns/stream-from-file)

    **Goal**: Process a file that is too large to fit into memory.
    This pattern demonstrates the core memory efficiency of streams, a critical feature for handling large datasets.

3.  #### [Turn a Paginated API into a Single Stream](./patterns/stream-from-paginated-api)

    **Goal**: Convert a paginated API into a continuous, easy-to-use stream.
    Learn how to solve a common and complex data-sourcing problem with an elegant, declarative solution.

#### Part 2: Transforming Pipeline Data (Transformers)

1.  #### [Process Items Concurrently](./patterns/stream-process-concurrently)

    **Goal**: Perform an asynchronous action for each item in a stream with controlled concurrency.
    This is the workhorse pattern for most real-world pipelines, showing how to balance performance and stability.

2.  #### [Automatically Retry Failed Operations](./patterns/stream-retry-on-failure)

    **Goal**: Build a self-healing pipeline that can automatically retry failed processing steps.
    Learn how to leverage `Schedule` to make your data pipelines resilient to transient failures.

3.  #### [Process Items in Batches](./patterns/stream-process-in-batches)

    **Goal**: Group items into chunks for efficient bulk operations, like database inserts.
    This is a common and powerful performance optimization technique for interacting with external services.

#### Part 3: Running a Pipeline (Sinks)

1.  #### [Collect All Results into a List](./patterns/stream-collect-results)

    **Goal**: Run a pipeline and gather all of its results into an in-memory array.
    This is the simplest way to get a final, consolidated result from a finite stream.

2.  #### [Run a Pipeline for its Side Effects](./patterns/stream-run-for-effects)

    **Goal**: Execute a pipeline for its effects without collecting the results, saving memory.
    Ideal for "fire and forget" tasks like writing each item to a database or sending notifications.

3.  #### [Manage Resources Safely in a Pipeline](./patterns/stream-manage-resources)

    **Goal**: Ensure resources like file handles or connections are safely acquired and always released, even on failure.
    This pattern reveals the heart of resource safety in Effect and is key to building truly robust pipelines.