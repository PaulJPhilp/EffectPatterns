---
id: schema-ai-parsing-streaming-validation
title: Validating Streaming AI Responses
category: ai-schemas
skill: advanced
tags:
  - schema
  - ai
  - streaming
  - validation
  - real-time
  - incremental
lessonOrder: 5
---

# Problem

An LLM streams JSON responses back in chunks. You need to validate incrementally as data arrives—catching schema violations early without waiting for the full response, updating UI in real-time, and gracefully handling mid-stream failures. Standard parsing waits for complete JSON and fails atomically.

# Solution

```typescript
import {
  Effect,
  Schema,
  Stream,
  Channel,
} from "effect"
import { Anthropic } from "@anthropic-ai/sdk"

// 1. Define the streamed data type
const StreamedThought = Schema.Struct({
  id: Schema.String,
  content: Schema.String,
  completed: Schema.Boolean.pipe(Schema.withDefault(false)),
})

type StreamedThought = typeof StreamedThought.Type

// 2. JSON stream parser
const jsonStreamParser = (
  chunks: AsyncIterable<string>
): Stream.Stream<unknown> =>
  Stream.fromAsyncIterable(chunks, (e) =>
    new Error(String(e))
  ).pipe(
    Stream.fold(
      { buffer: "", depth: 0, bracketStack: [] },
      (state, chunk) => {
        state.buffer += chunk

        const results: unknown[] = []
        let i = 0

        while (i < state.buffer.length) {
          const char = state.buffer[i]

          if (char === "{") {
            state.depth++
            state.bracketStack.push("}")
          } else if (char === "}") {
            state.depth--
            state.bracketStack.pop()

            if (state.depth === 0) {
              // Found complete object
              const jsonStr = state.buffer.substring(
                0,
                i + 1
              )

              try {
                results.push(JSON.parse(jsonStr))
              } catch (e) {
                console.warn(
                  `Malformed JSON at position ${i}: ${e}`
                )
              }

              state.buffer = state.buffer.substring(
                i + 1
              )
              i = -1 // Reset counter
            }
          }

          i++
        }

        return {
          state,
          emitted: results,
        }
      },
      (state) => {
        // Handle any remaining data on stream end
        if (
          state.buffer.trim().length > 0 &&
          state.depth === 0
        ) {
          try {
            return [JSON.parse(state.buffer)]
          } catch (e) {
            console.warn(
              `Final buffer not valid JSON: ${state.buffer}`
            )
            return []
          }
        }
        return []
      }
    )
  )

// 3. Incremental validator
const validateStreamed = (object: unknown) =>
  Effect.gen(function* () {
    const decoder = Schema.decodeUnknown(StreamedThought)

    const result = yield* decoder(object).pipe(
      Effect.mapError((error) => ({
        _tag: "ValidationError" as const,
        message: `Invalid thought: ${error.message}`,
        raw: object,
      }))
    )

    return result
  })

// 4. Stream handler with live updates
const handleStreamedThoughts = (
  chunks: AsyncIterable<string>,
  onThought: (thought: StreamedThought) => void,
  onError: (error: Error) => void
) =>
  Effect.gen(function* () {
    const stream = jsonStreamParser(chunks)

    yield* stream.pipe(
      Stream.mapEffect((json) =>
        validateStreamed(json).pipe(
          Effect.tap((thought) =>
            Effect.sync(() => onThought(thought))
          ),
          Effect.catchTag("ValidationError", (error) =>
            Effect.sync(() => {
              console.warn(
                `Validation warning: ${error.message}`
              )
              // Continue stream on validation error
            })
          )
        )
      ),
      Stream.catchAll((error) =>
        Effect.sync(() => onError(error))
      ),
      Stream.runDrain
    )
  })

// 5. Streaming with chunked validation
const callLLMStreaming = (
  prompt: string,
  onChunk: (chunk: StreamedThought) => void
) =>
  Effect.gen(function* () {
    const client = new Anthropic()

    const stream = await client.messages.stream({
      model: "claude-3-5-sonnet-20241022",
      max_tokens: 2048,
      messages: [
        { role: "user", content: prompt },
      ],
    })

    // Convert to async iterable of text chunks
    const textStream = async function* () {
      for await (const event of stream) {
        if (
          event.type === "content_block_delta" &&
          event.delta.type === "text_delta"
        ) {
          yield event.delta.text
        }
      }
    }

    yield* handleStreamedThoughts(
      textStream(),
      onChunk,
      (error) => {
        console.error(
          "Stream error:",
          error.message
        )
      }
    )
  })

// 6. Real-time UI updates (conceptual)
interface UIState {
  thoughts: StreamedThought[]
  isStreaming: boolean
  errorCount: number
}

const streamUI = (prompt: string) =>
  Effect.gen(function* () {
    const state: UIState = {
      thoughts: [],
      isStreaming: true,
      errorCount: 0,
    }

    yield* callLLMStreaming(prompt, (thought) => {
      // Update UI in real-time
      state.thoughts.push(thought)
      console.log(`[${thought.id}] ${thought.content}`)

      if (thought.completed) {
        console.log("✅ Thought complete")
      }
    })

    state.isStreaming = false
    return state
  })

// 7. Usage
const examplePrompt =
  'Think step-by-step about how Effect types work'

Effect.runPromise(streamUI(examplePrompt))
  .then((finalState) => {
    console.log(
      `Collected ${finalState.thoughts.length} thoughts`
    )
    console.log(
      `Stream finished. Errors: ${finalState.errorCount}`
    )
  })
  .catch((error) => {
    console.error("Stream failed:", error.message)
  })
```

# Why This Works

| Concept | Explanation |
|---------|-------------|
| JSON stream parser | Detects complete objects mid-stream without waiting for EOF |
| Incremental validation | Schema check on each object as it arrives, errors don't stop stream |
| Real-time UI updates | Call `onThought` callback for live rendering |
| Error recovery | Invalid objects logged but stream continues |
| `Stream.runDrain` | Ensures all effects run and side effects fire |

# When to Use

- Streaming LLM responses (Claude, OpenAI streaming API)
- Real-time chat applications
- Long-running LLM tasks with progress feedback
- When you want to display partial results
- Validating incrementally instead of waiting for complete response

# Related Patterns

- [Basic AI Response Parsing](./basic.md)
- [Parsing Partial/Incomplete Responses](./partial-responses.md)
- [Retry Strategies for Parse Failures](./retry-on-failure.md)
- [Handling Malformed AI Outputs](./error-recovery.md)
