---
title: "Stream Pattern 1: Transform Streams with Map and Filter"
id: stream-pattern-map-filter-transformations
skillLevel: beginner
applicationPatternId: streams
summary: >-
  Use Stream.map and Stream.filter to transform and select stream elements,
  enabling data pipelines that reshape and filter data in flight.
tags:
  - streams
  - transformations
  - map
  - filter
  - data-pipeline
  - functional-programming
rule:
  description: >-
    Use map and filter combinators to transform stream elements declaratively,
    creating pipelines that reshape data without materializing intermediate results.
related:
  - process-streaming-data-with-stream
  - stream-from-iterable
  - stream-collect-results
author: effect_website
---

## Guideline

Use `Stream.map` and `Stream.filter` to transform streams:

- **map**: Transform each element (1‚Üí1)
- **filter**: Keep elements matching predicate (N‚ÜíN, discards some)
- **Chain**: Compose multiple operations
- **Lazy**: Elements transformed on demand (no buffering)

Pattern: `stream.pipe(Stream.map(...), Stream.filter(...))`

---

## Rationale

Streaming data transformations without map/filter create problems:

- **Buffering**: Must collect all data before transforming
- **Code verbosity**: Manual loops for each transformation
- **Memory usage**: Large intermediate arrays
- **Composability**: Hard to chain operations

Map/filter enable:

- **Lazy evaluation**: Transform on-demand
- **Composable**: Chain operations naturally
- **Memory efficient**: No intermediate collections
- **Expressive**: Declare intent clearly

Real-world example: Processing logs
- **Without map/filter**: Collect logs, filter by level, map to objects, transform fields
- **With map/filter**: `logStream.pipe(Stream.filter(...), Stream.map(...))`

---

## Good Example

This example demonstrates transforming a stream of raw data through multiple stages.

```typescript
import { Stream, Effect, Chunk } from "effect";

interface RawLogEntry {
  readonly timestamp: string;
  readonly level: string;
  readonly message: string;
}

interface ProcessedLog {
  readonly date: Date;
  readonly severity: "low" | "medium" | "high";
  readonly normalizedMessage: string;
}

// Create a stream of raw log entries
const createLogStream = (): Stream.Stream<RawLogEntry> =>
  Stream.fromIterable([
    { timestamp: "2025-12-17T09:00:00Z", level: "DEBUG", message: "App starting" },
    { timestamp: "2025-12-17T09:01:00Z", level: "INFO", message: "Connected to DB" },
    { timestamp: "2025-12-17T09:02:00Z", level: "ERROR", message: "Query timeout" },
    { timestamp: "2025-12-17T09:03:00Z", level: "DEBUG", message: "Retry initiated" },
    { timestamp: "2025-12-17T09:04:00Z", level: "WARN", message: "Connection degraded" },
    { timestamp: "2025-12-17T09:05:00Z", level: "INFO", message: "Recovered" },
  ]);

// Transform: Parse timestamp
const parseTimestamp = (entry: RawLogEntry): RawLogEntry => ({
  ...entry,
  timestamp: entry.timestamp, // Already ISO, but could parse here
});

// Transform: Map log level to severity
const mapSeverity = (level: string): "low" | "medium" | "high" => {
  if (level === "DEBUG" || level === "INFO") return "low";
  if (level === "WARN") return "medium";
  return "high";
};

// Transform: Normalize message
const normalizeMessage = (message: string): string =>
  message.toLowerCase().trim();

// Filter: Keep only important logs
const isImportant = (entry: RawLogEntry): boolean => {
  return entry.level !== "DEBUG";
};

// Main pipeline
const program = Effect.gen(function* () {
  console.log(`\n[STREAM] Processing log stream with map/filter\n`);

  // Create and transform stream
  const transformedStream = createLogStream().pipe(
    // Filter: Keep only non-debug logs
    Stream.filter((entry) => {
      const important = isImportant(entry);
      console.log(
        `[FILTER] ${entry.level} ‚Üí ${important ? "‚úì kept" : "‚úó filtered out"}`
      );
      return important;
    }),

    // Map: Extract date
    Stream.map((entry) => {
      const date = new Date(entry.timestamp);
      console.log(`[MAP-1] Parsed date: ${date.toISOString()}`);
      return { ...entry, parsedDate: date };
    }),

    // Map: Normalize and map severity
    Stream.map((entry) => {
      const processed: ProcessedLog = {
        date: entry.parsedDate,
        severity: mapSeverity(entry.level),
        normalizedMessage: normalizeMessage(entry.message),
      };
      console.log(
        `[MAP-2] Transformed: ${entry.level} ‚Üí ${processed.severity}`
      );
      return processed;
    })
  );

  // Collect all transformed logs
  const results = yield* transformedStream.pipe(
    Stream.runCollect
  );

  console.log(`\n[RESULTS]`);
  console.log(`  Total logs: ${results.length}`);

  Chunk.forEach(results, (log) => {
    console.log(
      `  - [${log.severity.toUpperCase()}] ${log.date.toISOString()}: ${log.normalizedMessage}`
    );
  });
});

Effect.runPromise(program);
```

Output shows lazy evaluation and filtering:
```
[STREAM] Processing log stream with map/filter

[FILTER] DEBUG ‚Üí ‚úó filtered out
[FILTER] INFO ‚Üí ‚úì kept
[MAP-1] Parsed date: 2025-12-17T09:01:00.000Z
[MAP-2] Transformed: INFO ‚Üí low
[FILTER] ERROR ‚Üí ‚úì kept
[MAP-1] Parsed date: 2025-12-17T09:02:00.000Z
[MAP-2] Transformed: ERROR ‚Üí high
...

[RESULTS]
  Total logs: 5
  - [LOW] 2025-12-17T09:01:00.000Z: connected to db
  - [HIGH] 2025-12-17T09:02:00.000Z: query timeout
  ...
```

---

## Advanced: Conditional Mapping

Apply different transformations based on conditions:

```typescript
const conditionalMap = <A, B>(
  stream: Stream.Stream<A>,
  predicate: (a: A) => boolean,
  mapTrue: (a: A) => B,
  mapFalse: (a: A) => B
): Stream.Stream<B> =>
  stream.pipe(
    Stream.map((item) =>
      predicate(item) ? mapTrue(item) : mapFalse(item)
    )
  );

// Example: Different handling for high/low severity logs
const conditionalLogProcessing = createLogStream().pipe(
  conditionalMap(
    (entry) => entry.level === "ERROR" || entry.level === "WARN",
    // Transform errors to alerts
    (entry) => ({
      type: "alert" as const,
      level: entry.level,
      message: `‚ö† ${entry.message}`,
    }),
    // Transform info logs to metrics
    (entry) => ({
      type: "metric" as const,
      level: entry.level,
      message: `üìä ${entry.message}`,
    })
  )
);
```

---

## Advanced: Stateful Transformations with scan

Maintain state while transforming:

```typescript
interface LineMetrics {
  readonly lineNumber: number;
  readonly text: string;
  readonly charCount: number;
  readonly cumulativeChars: number;
}

const logStreamWithMetrics = (
  logStream: Stream.Stream<RawLogEntry>
): Stream.Stream<LineMetrics> =>
  logStream.pipe(
    Stream.scan(
      { lineNumber: 0, cumulativeChars: 0 },
      (acc, entry) => {
        const charCount = entry.message.length;
        const newAcc = {
          lineNumber: acc.lineNumber + 1,
          cumulativeChars: acc.cumulativeChars + charCount,
        };

        return [
          newAcc,
          {
            lineNumber: newAcc.lineNumber,
            text: entry.message,
            charCount,
            cumulativeChars: newAcc.cumulativeChars,
          },
        ];
      }
    )
  );

// Use it
const metricsProgram = logStreamWithMetrics(createLogStream()).pipe(
  Stream.runForEach((metric) =>
    Effect.log(
      `Line ${metric.lineNumber}: ${metric.charCount} chars (cumulative: ${metric.cumulativeChars})`
    )
  )
);
```

---

## Advanced: Filter with Side Effects

Perform checks with logging during filtering:

```typescript
interface FilterMetrics {
  readonly total: number;
  readonly kept: number;
  readonly filtered: number;
}

const filterWithMetrics = <A>(
  stream: Stream.Stream<A>,
  predicate: (a: A) => boolean
): Stream.Stream<A> & { metrics: () => Effect.Effect<FilterMetrics> } => {
  const metrics = { total: 0, kept: 0, filtered: 0 };

  const filtered = stream.pipe(
    Stream.tap((item) =>
      Effect.sync(() => {
        metrics.total++;
      })
    ),
    Stream.filter((item) => {
      const result = predicate(item);
      if (result) {
        metrics.kept++;
      } else {
        metrics.filtered++;
      }
      return result;
    })
  );

  return Object.assign(filtered, {
    metrics: () => Effect.succeed(metrics),
  });
};
```

---

## Advanced: Chained Transformations Pipeline

Build complex pipelines with multiple stages:

```typescript
interface DataPoint {
  readonly id: number;
  readonly value: number;
  readonly timestamp: Date;
}

const analyticsPipeline = (
  dataStream: Stream.Stream<DataPoint>
): Stream.Stream<{
  readonly id: number;
  readonly original: number;
  readonly normalized: number;
  readonly category: string;
}> =>
  dataStream.pipe(
    // Stage 1: Filter out nulls/zeros
    Stream.filter((point) => point.value !== 0),

    // Stage 2: Normalize to 0-100 range
    Stream.map((point) => ({
      ...point,
      normalized: Math.min(100, Math.max(0, point.value)),
    })),

    // Stage 3: Categorize by value
    Stream.map((point) => ({
      id: point.id,
      original: point.value,
      normalized: point.normalized,
      category:
        point.normalized < 33 ? "low" :
        point.normalized < 67 ? "medium" :
        "high",
    })),

    // Stage 4: Filter out only "low" for further processing
    Stream.filter((point) => point.category !== "low")
  );

// Usage
const pipelineExample = analyticsPipeline(
  Stream.fromIterable([
    { id: 1, value: 10, timestamp: new Date() },
    { id: 2, value: 50, timestamp: new Date() },
    { id: 3, value: 0, timestamp: new Date() },
    { id: 4, value: 95, timestamp: new Date() },
  ])
).pipe(
  Stream.tap((item) =>
    Effect.log(`Processed: ${item.id} ‚Üí ${item.category}`)
  ),
  Stream.runDrain
);
```

---

## When to Use This Pattern

‚úÖ **Use map/filter when:**

- Transforming stream elements
- Selecting subset of elements
- Chaining multiple operations
- Building data pipelines
- Processing without buffering
- Declarative data transformations

‚ö†Ô∏è **Trade-offs:**

- Each operation adds small overhead
- Long chains can be hard to debug
- Can't introspect intermediate values easily
- Errors propagate through entire pipeline

---

## Map vs Filter vs Fold

| Operator | Input | Output | Use Case |
| --- | --- | --- | --- |
| **map** | N elements | N elements (transformed) | Transform each element |
| **filter** | N elements | M elements (M ‚â§ N) | Select subset |
| **fold** | N elements | 1 element | Aggregate all into one |
| **scan** | N elements | N elements (stateful) | Stateful transformation |

---

## See Also

- [Process Streaming Data with Stream](./process-streaming-data-with-stream.mdx) - Stream basics
- [Stream Pattern 2: Merge & Combine Streams](./stream-pattern-merge-combine.mdx) - Combining streams
- [Stream from Iterable](./stream-from-iterable.mdx) - Creating streams
- [Stream Collect Results](./stream-collect-results.mdx) - Collecting stream output
