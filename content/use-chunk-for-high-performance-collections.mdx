---
title: "Use Chunk for High-Performance Collections"
id: "use-chunk-for-high-performance-collections"
skillLevel: "intermediate"
useCase:
  - "Core Concepts"
summary: "Use Chunk&lt;A&gt; as a high-performance, immutable alternative to JavaScript's Array, especially for data processing pipelines."
tags:
  - "chunk"
  - "collections"
  - "performance"
  - "immutable"
  - "data"
rule:
  description: "Prefer Chunk over Array for immutable collection operations within data processing pipelines for better performance."
related:
  - "process-streaming-data-with-stream"
author: "effect_website"
---

## Guideline

For collections that will be heavily transformed with immutable operations (e.g., `map`, `filter`, `append`), use ``Chunk<A>``. ``Chunk`` is Effect's implementation of a persistent and chunked vector that provides better performance than native arrays for these use cases.

---

## Rationale

JavaScript's `Array` is a mutable data structure. Every time you perform an "immutable" operation like `[...arr, newItem]` or `arr.map(...)`, you are creating a brand new array and copying all the elements from the old one. For small arrays, this is fine. For large arrays or in hot code paths, this constant allocation and copying can become a performance bottleneck.

`Chunk` is designed to solve this. It's an immutable data structure that uses structural sharing internally. When you append an item to a `Chunk`, it doesn't re-copy the entire collection. Instead, it creates a new `Chunk` that reuses most of the internal structure of the original, only allocating memory for the new data. This makes immutable appends and updates significantly faster.

`Stream` uses `Chunk` internally for this very reason. You should use `Chunk` when you are building data processing pipelines or need to work with collections in a highly performant, immutable way.

---

## Good Example

This example shows how to create and manipulate a `Chunk`. The API is very similar to `Array`, but the underlying performance characteristics for these immutable operations are superior.

```typescript
import { Chunk } from "effect";

// Create a Chunk from an array
let numbers = Chunk.fromIterable([1, 2, 3, 4, 5]);

// Append a new element. This is much faster than [...arr, 6] on large collections.
numbers = Chunk.append(numbers, 6);

// Prepend an element.
numbers = Chunk.prepend(numbers, 0);

// Take the first 3 elements
const firstThree = Chunk.take(numbers, 3);

// Convert back to an array when you need to interface with other libraries
const finalArray = Chunk.toReadonlyArray(firstThree);

console.log(finalArray); // [0, 1, 2]
```

---

## Anti-Pattern

Using standard JavaScript arrays for heavy, immutable data processing pipelines, especially within a `Stream`. This can lead to unnecessary memory allocation and garbage collection pressure.

```typescript
import { Effect, Stream } from "effect";

const numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];

// ❌ This works, but can be less performant.
// Inside this stream, each `map` and `filter` creates new intermediate arrays.
const program = Stream.fromIterable(numbers).pipe(
  Stream.map((n) => n * 2),
  Stream.filter((n) => n > 10),
  Stream.runCollect, // This will collect the results into a Chunk anyway
);

// ✅ Better: If you start with a Chunk, the operations can be more efficient.
const programWithChunk = Stream.fromChunk(Chunk.fromIterable(numbers)).pipe(
  Stream.map((n) => n * 2),
  Stream.filter((n) => n > 10),
  Stream.runCollect,
);
```