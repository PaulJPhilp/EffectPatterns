---
title: "Automatically Retry Failed Operations"
id: "stream-retry-on-failure"
skillLevel: "intermediate"
useCase:
  - "Building Data Pipelines"
summary: "Build a self-healing pipeline that can automatically retry failed processing steps using a configurable backoff strategy."
tags:
  - "stream"
  - "retry"
  - "resilience"
  - "error-handling"
  - "schedule"
rule:
  description: "Compose a Stream with the .retry(Schedule) operator to automatically recover from transient failures."
author: "PaulJPhilp"
related:
  - "stream-process-concurrently"
  - "handle-api-errors"
---

## Guideline

To make a data pipeline resilient to transient failures, apply the `.retry(Schedule)` operator to the `Stream`.

---

## Rationale

Real-world systems are unreliable. Network connections drop, APIs return temporary `503` errors, and databases can experience deadlocks. A naive pipeline will fail completely on the first sign of trouble. A resilient pipeline, however, can absorb these transient errors and heal itself.

The `retry` operator, combined with the `Schedule` module, provides a powerful and declarative way to build this resilience:

1.  **Declarative Resilience**: Instead of writing complex `try/catch` loops with manual delay logic, you declaratively state *how* the pipeline should retry. For example, "retry 3 times, with an exponential backoff starting at 100ms."
2.  **Separation of Concerns**: Your core pipeline logic remains focused on the "happy path." The retry strategy is a separate, composable concern that you apply to the entire stream.
3.  **Rich Scheduling Policies**: `Schedule` is incredibly powerful. You can create schedules based on a fixed number of retries, exponential backoff, jitter (to avoid thundering herd problems), or even combinations of these.
4.  **Prevents Cascading Failures**: By handling temporary issues at the source, you prevent a small, transient glitch from causing a complete failure of your entire application.

---

## Good Example

This example simulates an API that fails the first two times it's called. The stream processes a list of IDs, and the `retry` operator ensures that the failing operation for `id: 2` is automatically retried until it succeeds.

`<Example path="./src/stream-retry-on-failure.ts" />`

## Anti-Pattern

The anti-pattern is to either have no retry logic at all, or to write manual, imperative retry loops inside your processing function.

````typescript
import { Effect, Stream } from 'effect';
// ... same mock processItem function ...

const ids = [1, 2, 3];

const program = Stream.fromIterable(ids).pipe(
  // No retry logic. The entire stream will fail when item 2 fails.
  Stream.mapEffect(processItem, { concurrency: 1 }),
  Stream.runDrain
);

Effect.runPromise(program).catch((error) => {
  console.error('Pipeline failed:', error);
});
/*
Output:
... level=INFO msg="Attempting to process item 1..."
... level=INFO msg="Attempting to process item 2..."
... level=INFO msg="Item 2 failed, attempt 1."
Pipeline failed: Error: API is temporarily down
*/
````

This "fail-fast" approach is brittle. A single, temporary network blip would cause the entire pipeline to terminate, even if subsequent items could have been processed successfully. While manual retry logic inside `processItem` is possible, it pollutes the core logic with concerns about timing and attempt counting, and is far less composable and reusable than a `Schedule`.